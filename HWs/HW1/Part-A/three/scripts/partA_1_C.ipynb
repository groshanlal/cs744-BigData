{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BEGIN IMPORTS\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "# END IMPORTS\n",
    "\n",
    "xml_data = open('../MR/job_1506646956217_0035_conf.xml').read() #Loading the raw XML data\n",
    "\n",
    "def xml2df(xml_data):\n",
    "    root = ET.XML(xml_data) # element tree\n",
    "    all_records = [] #This is our record list which we will convert into a dataframe\n",
    "    for i, child in enumerate(root): #Begin looping through our root tree\n",
    "        record = {} #Place holder for our record\n",
    "        for subchild in child: #iterate through the subchildren to user-agent, Ex: ID, String, Description.\n",
    "            record[subchild.tag] = subchild.text #Extract the text create a new dictionary key, value pair\n",
    "            all_records.append(record) #Append this record to all_records.\n",
    "    return pd.DataFrame(all_records) #return records as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydf = xml2df(xml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         dfs.block.access.token.lifetime\n",
       "1                         dfs.block.access.token.lifetime\n",
       "2                         dfs.block.access.token.lifetime\n",
       "3                         dfs.block.access.token.lifetime\n",
       "4                                       hive.skewjoin.key\n",
       "5                                       hive.skewjoin.key\n",
       "6                                       hive.skewjoin.key\n",
       "7                                       hive.skewjoin.key\n",
       "8                                       hive.skewjoin.key\n",
       "9                        hive.index.compact.binary.search\n",
       "10                       hive.index.compact.binary.search\n",
       "11                       hive.index.compact.binary.search\n",
       "12                       hive.index.compact.binary.search\n",
       "13                       hive.index.compact.binary.search\n",
       "14                                mapreduce.map.log.level\n",
       "15                                mapreduce.map.log.level\n",
       "16                                mapreduce.map.log.level\n",
       "17                                mapreduce.map.log.level\n",
       "18       dfs.namenode.lazypersist.file.scrub.interval.sec\n",
       "19       dfs.namenode.lazypersist.file.scrub.interval.sec\n",
       "20       dfs.namenode.lazypersist.file.scrub.interval.sec\n",
       "21       dfs.namenode.lazypersist.file.scrub.interval.sec\n",
       "22                                file.bytes-per-checksum\n",
       "23                                file.bytes-per-checksum\n",
       "24                                file.bytes-per-checksum\n",
       "25                                file.bytes-per-checksum\n",
       "26               mapreduce.client.completion.pollinterval\n",
       "27               mapreduce.client.completion.pollinterval\n",
       "28               mapreduce.client.completion.pollinterval\n",
       "29               mapreduce.client.completion.pollinterval\n",
       "                              ...                        \n",
       "5385            mapreduce.reduce.markreset.buffer.percent\n",
       "5386            mapreduce.reduce.markreset.buffer.percent\n",
       "5387            mapreduce.reduce.markreset.buffer.percent\n",
       "5388            mapreduce.reduce.markreset.buffer.percent\n",
       "5389      mapreduce.shuffle.connection-keep-alive.timeout\n",
       "5390      mapreduce.shuffle.connection-keep-alive.timeout\n",
       "5391      mapreduce.shuffle.connection-keep-alive.timeout\n",
       "5392      mapreduce.shuffle.connection-keep-alive.timeout\n",
       "5393    hadoop.security.kms.client.encrypted.key.cache...\n",
       "5394    hadoop.security.kms.client.encrypted.key.cache...\n",
       "5395    hadoop.security.kms.client.encrypted.key.cache...\n",
       "5396    hadoop.security.kms.client.encrypted.key.cache...\n",
       "5397                hive.server2.map.fair.scheduler.queue\n",
       "5398                hive.server2.map.fair.scheduler.queue\n",
       "5399                hive.server2.map.fair.scheduler.queue\n",
       "5400                hive.server2.map.fair.scheduler.queue\n",
       "5401                hive.server2.map.fair.scheduler.queue\n",
       "5402                                     hive.txn.manager\n",
       "5403                                     hive.txn.manager\n",
       "5404                                     hive.txn.manager\n",
       "5405                                     hive.txn.manager\n",
       "5406                                     hive.txn.manager\n",
       "5407                              hadoop.registry.zk.root\n",
       "5408                              hadoop.registry.zk.root\n",
       "5409                              hadoop.registry.zk.root\n",
       "5410                              hadoop.registry.zk.root\n",
       "5411                                s3.stream-buffer-size\n",
       "5412                                s3.stream-buffer-size\n",
       "5413                                s3.stream-buffer-size\n",
       "5414                                s3.stream-buffer-size\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tez_file = open('/home/ehsan/MyWork/CS_744/HWs/HW1/partA/one/tpcds_query85_tez.out').read()\n",
    "# mydf = pd.DataFrame(data=0, index=[\"map\",\"reduce\"],columns=[\"0\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time = 0;\n",
    "maps = [0]\n",
    "reduces = [0]\n",
    "for line in tez_file.splitlines():\n",
    "    if \"Map\" not in line:\n",
    "        continue;\n",
    "    mp  = 0;\n",
    "    rdc = 0;\n",
    "#     print line;\n",
    "    for entry in line.split(\"\\t\"):\n",
    "        if len(entry) <1:\n",
    "            continue;\n",
    "        value = (entry.split(\":\")[1]).split(\"/\")[0].split(\"(\")[0];\n",
    "        if \"-\" in value:\n",
    "            continue;\n",
    "        if \"Map\" in entry:\n",
    "            mp += int(value)\n",
    "        elif \"Reduce\" in entry:\n",
    "            rdc += int(value)\n",
    "#         print (mp,rdc)\n",
    "\n",
    "    time +=1;\n",
    "    \n",
    "    maps.append(mp-sum(maps))\n",
    "    reduces.append(rdc-sum(reduces))\n",
    "#     if time > 10:\n",
    "#         break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print reduces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
