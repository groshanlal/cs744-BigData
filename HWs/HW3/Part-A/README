Synchronous SGD
In this part we implemented the synchronous SGD algorithm based on two example codes provided. One example code shows how the input queues work in tensor flow environment. The second example code shows a simple synchronous update example. 
We based our implementation on the second example code and added three main functions to it: get_datapoint_iter, calc_gradient, and calc_precision. These functions handle different aspects of our synchronous SGD from getting the training/testing dataset, pre-process them and finally use them in training procedure and testing
The function get_datapoint_iter,is responsible for creating and configuring the input queues. It gets a list of tf recordqueue file names and an integer batch_size parameter as input. Then it initialize input queues based on the filenames, cast the data read from output and finally use shuffle_batch method for generating batches of input. Each batch of data points is consist of a batch sparse tensor with dense shape of [batch_size,num_features].
The function, calc_gradient, gets the training data parameters and labels as well as the model parameters as input. Then based on the given equation for calculating the gradient it calculates the gradient of the given batch. It must be noted that as the training data parameters are extremely sparse tensors, we used tensorflow’s sparse arithmetic library to calculate sparse-dense multiplications required for gradients. Using sparse arithmetic library has advantages in both memory and cpu power used for computation. In order to profile our code to find bottlenecks in our code, we used tensorflow’s profiling tool and profiled our codes runtime during on training cycle (figure 1). As shown in the profiling and the detailed result in table 1, most of the computation in this part is spent on last sparse_tensor_dense_matmul operator. This big runtime is directly caused by the large output of the operation which is a dense [batch_size,num_features] tensor where each row is the gradient related to a training datapoint. Although this large matrix is aggregated and reduced in next operator, it is still the most expensive part of calculating gradient computationally. This large matrix will not have direct effect on communication cost of algorithm due to the aggregation operator placed before transmission.   
Finally the function, calc_precision is responsible for reading the testing datasets, calculate the prediction based on learned parameter and finally report the precision to console.  



Asynchronous SGD
In this part we talk about our implementation of asynchronous SGD. Like the previous part we used three functions to handle different aspects of training. However in this part we based on design on the provided example Async code. 
