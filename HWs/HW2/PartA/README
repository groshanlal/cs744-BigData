Question 1:
	in this part we used the readstream in SparkStream library to read the csv input files from the indicated directory. The code here is desgined to read files with pattern *.csv only to ignore the _COPYING_ files that is generated by the HDFS during copy of data (this _COPYING_ file is generated due to process of copying datasets that is used to emulate the streaming process). 
	Next we used groupby method accompanied with "window" to group the input data based on timestamp associated with them. Finally we aggregate each group and sum the results. 
	As requested by the assignemnt, at the end, we print output periodicaly to consule.

Question 2:
	the input method here is same as above. however during process instead of grouping we used where clause to filter only mention interaction, and we used select to select userB column. Finally in order to ensure that output data is dumped with period of 10 seconds we used "trigger" with writestream.

Question 3:
	the only difference here is reading from a parquet file instead of csv. we also use the trigger construct to ensure 5 seconds period of dumping. Also to calcualte the intended resutls we used groupby and aggregate methods to count the number of times each user is mentioned in the tweets.   
