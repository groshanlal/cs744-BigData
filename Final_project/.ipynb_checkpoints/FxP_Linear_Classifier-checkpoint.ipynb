{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import FixedPoint\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "alpha = 0.01\n",
    "s_batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_prec = (10,10)\n",
    "inter_prec = (10,10)\n",
    "output_prec = (10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "\t# compute and return the sigmoid activation value for a\n",
    "\t# given input value\n",
    "    family = x.get_value(0,0).family\n",
    "    if type(x) == type(pd.DataFrame([0])):\n",
    "        return x.applymap(\n",
    "            lambda v: \n",
    "            FixedPoint.FXnum(\n",
    "                1/(1+exp(-float(v))),\n",
    "                family\n",
    "            ) \n",
    "        )\n",
    "    elif type(x) == type(pd.Series([0])):\n",
    "        return x.map(\n",
    "            lambda v: \n",
    "            FixedPoint.FXnum(\n",
    "                1/(1+exp(-float(v))),\n",
    "                family\n",
    "            ) \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(X, y, batchSize):\n",
    "\t# loop over our dataset `X` in mini-batches of size `batchSize`\n",
    "\tfor i in np.arange(0, X.shape[0], batchSize):\n",
    "\t\t# yield a tuple of the current batched data and labels\n",
    "\t\tyield (X[i:i + batchSize], y[i:i + batchSize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_gradint(X,y,W):\n",
    "    # take the dot product between our features `X` and the\n",
    "    # weight matrix `W`, then pass this value through the\n",
    "    # sigmoid activation function, thereby giving us our\n",
    "    # predictions on the dataset\n",
    "    preds = sigmoid_activation(X.dot(W))\n",
    "    \n",
    "    # now that we have our predictions, we need to determine\n",
    "    # our `error`, which is the difference between our predictions\n",
    "    # and the true values\n",
    "    error = preds - y\n",
    " \n",
    "    # given our `error`, we can compute the total loss value as\n",
    "    # the sum of squared loss -- ideally, our loss should\n",
    "    # decrease as we continue training\n",
    "    loss = (error ** 2).sum()\n",
    "    return (X.T.dot(error) / X.shape[0],loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate a 2-class classification problem with 250 data points,\n",
    "# where each data point is a 2D feature vector\n",
    "(X, y) = make_blobs(n_samples=250, n_features=n_features, centers=2,\n",
    "\tcluster_std=10.0, random_state=95)\n",
    "\n",
    "# conver to pandas dataframe\n",
    "y = pd.DataFrame(y)\n",
    "\n",
    "# insert a column of 1's as the first entry in the feature\n",
    "# vector -- this args[\"epochs\"]is a little trick that allows us to treat\n",
    "# the bias as a trainable parameter *within* the weight matrix\n",
    "# rather than an entirely separate variable\n",
    "X = pd.DataFrame(np.c_[np.ones((X.shape[0])), X])\n",
    " \n",
    "# initialize our weight matrix such it has the same number of\n",
    "# columns as our input features\n",
    "W = pd.DataFrame(np.random.uniform(size=(X.shape[1],)))\n",
    " \n",
    "# initialize a list to store the loss value for each epoch\n",
    "lossHistory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# floating point data\n",
    "f_X = X.applymap(\n",
    "    lambda v: FixedPoint.FXnum(\n",
    "            v,FixedPoint.FXfamily(n_bits=input_prec[0], n_intbits=input_prec[1])\n",
    "    )\n",
    ")\n",
    "f_W = W.applymap(\n",
    "    lambda v: FixedPoint.FXnum(\n",
    "            v,FixedPoint.FXfamily(n_bits=inter_prec[0], n_intbits=inter_prec[1])\n",
    "    )\n",
    ")\n",
    "f_y = y.applymap(\n",
    "    lambda v: FixedPoint.FXnum(\n",
    "            v,FixedPoint.FXfamily(n_bits=output_prec[0], n_intbits=output_prec[1])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradient,loss = calc_gradint(f_X,f_y,f_W)\n",
    "preds = sigmoid_activation(f_X.dot(f_W))\n",
    "error = preds - f_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-366.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-382.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>279.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-184.321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>264.466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>435.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-334.415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>35.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>125.974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>168.376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-94.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>205.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-60.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-488.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-266.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-373.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>176.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>128.481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-88.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>198.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>274.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>362.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-268.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>504.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-52.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>368.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-67.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>507.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-308.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>374.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-338.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>295.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-468.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>204.797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>342.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-181.314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0      8.693\n",
       "1   -366.414\n",
       "2    177.745\n",
       "3     70.413\n",
       "4    -17.322\n",
       "5   -382.629\n",
       "6       None\n",
       "7    279.744\n",
       "8       None\n",
       "9   -184.321\n",
       "10      None\n",
       "11   264.466\n",
       "12   435.187\n",
       "13  -334.415\n",
       "14    35.902\n",
       "15   125.974\n",
       "16   168.376\n",
       "17   -94.446\n",
       "18   205.759\n",
       "19      None\n",
       "20   -60.590\n",
       "21  -488.896\n",
       "22  -266.989\n",
       "23  -373.802\n",
       "24   176.605\n",
       "25   128.481\n",
       "26   -88.852\n",
       "27   198.520\n",
       "28   274.101\n",
       "29   362.284\n",
       "30      None\n",
       "31  -268.198\n",
       "32      None\n",
       "33   504.084\n",
       "34   -52.621\n",
       "35   368.964\n",
       "36   -67.667\n",
       "37   507.196\n",
       "38  -308.283\n",
       "39      None\n",
       "40   374.961\n",
       "41      None\n",
       "42      None\n",
       "43      None\n",
       "44  -338.007\n",
       "45      None\n",
       "46   295.968\n",
       "47  -468.571\n",
       "48   204.797\n",
       "49   342.936\n",
       "50  -181.314"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_X.T.dot(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-95db0d7523bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# term \"gradient descent\" by taking a small step towards a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# set of \"more optimal\" parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mf_W\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ehsan/anaconda2/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;31m# this makes sure that we are aligned like the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ehsan/anaconda2/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Another DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1228\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ehsan/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_combine_frame\u001b[0;34m(self, other, func, fill_value, level)\u001b[0m\n\u001b[1;32m   3567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3568\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3569\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_arith_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3571\u001b[0m         return self._constructor(result, index=new_index, columns=new_columns,\n",
      "\u001b[0;32m/home/ehsan/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_arith_op\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m   3536\u001b[0m                 \u001b[0mright\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_mask\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3538\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ehsan/anaconda2/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m             result = expressions.evaluate(op, str_rep, x, y,\n\u001b[0;32m-> 1170\u001b[0;31m                                           raise_on_error=True, **eval_kwargs)\n\u001b[0m\u001b[1;32m   1171\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0mxrav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ehsan/anaconda2/lib/python2.7/site-packages/pandas/computation/expressions.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, raise_on_error, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error,\n\u001b[0;32m--> 210\u001b[0;31m                          **eval_kwargs)\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_on_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ehsan/anaconda2/lib/python2.7/site-packages/pandas/computation/expressions.pyc\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b, raise_on_error, truediv, reversed, **eval_kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ehsan/anaconda2/lib/python2.7/site-packages/pandas/computation/expressions.pyc\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b, raise_on_error, **eval_kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ehsan/MyWork/CS_744/Final_project/FixedPoint.pyc\u001b[0m in \u001b[0;36m__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;34m\"\"\"Add another number\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_CastOrFail_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         return FXnum(family=self.family,\n\u001b[1;32m    364\u001b[0m                      scaled_value=(self.scaledval + other.scaledval))\n",
      "\u001b[0;32m/home/ehsan/MyWork/CS_744/Final_project/FixedPoint.pyc\u001b[0m in \u001b[0;36m_CastOrFail_\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;31m# Automatic casting from types other than FXnum is allowed:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFXnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ehsan/MyWork/CS_744/Final_project/FixedPoint.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, val, family, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             self.scaledval = kwargs.get('scaled_value',\n\u001b[0;32m--> 269\u001b[0;31m                                         int(val * family.scale))\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaledval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "# loop over the desired number of epochs\n",
    "for epoch in np.arange(0, epochs):\n",
    "\t# the gradient update is therefore the dot product between\n",
    "\t# the transpose of `X` and our error, scaled by the total\n",
    "\t# number of data points in `X`\n",
    "\tgradient,loss = calc_gradint(f_X,f_y,f_W)\n",
    "    \n",
    "    # keep track of the losses in each epoch\n",
    "\tlossHistory.append(loss)\n",
    "\n",
    "\t# in the update stage, all we need to do is nudge our weight\n",
    "\t# matrix in the negative direction of the gradient (hence the\n",
    "\t# term \"gradient descent\" by taking a small step towards a\n",
    "\t# set of \"more optimal\" parameters\n",
    "\tf_W += -alpha * gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation=1.0000; predicted_label=1, true_label=1.0\n",
      "activation=1.0000; predicted_label=1, true_label=1.0\n",
      "activation=1.0000; predicted_label=1, true_label=1.0\n",
      "activation=1.0000; predicted_label=1, true_label=1.0\n",
      "activation=0.0000; predicted_label=0, true_label=0.0\n",
      "activation=1.0000; predicted_label=1, true_label=1.0\n",
      "activation=1.0000; predicted_label=1, true_label=1.0\n",
      "activation=0.0000; predicted_label=0, true_label=0.0\n",
      "activation=0.0000; predicted_label=0, true_label=0.0\n",
      "activation=0.0000; predicted_label=0, true_label=0.0\n"
     ]
    }
   ],
   "source": [
    "# to demonstrate how to use our weight matrix as a classifier,\n",
    "# let's look over our a sample of training examples\n",
    "for i in np.random.choice(250, 10):\n",
    "\t# compute the prediction by taking the dot product of the\n",
    "\t# current feature vector with the weight matrix W, then\n",
    "\t# passing it through the sigmoid activation function\n",
    "\tactivation = sigmoid_activation(f_X.iloc[i].dot(f_W))\n",
    " \n",
    "\t# the sigmoid function is defined over the range y=[0, 1],\n",
    "\t# so we can use 0.5 as our threshold -- if `activation` is\n",
    "\t# below 0.5, it's class `0`; otherwise it's class `1`\n",
    "\tlabel = 0 if activation[0] < 0.5 else 1\n",
    " \n",
    "\t# show our output classification\n",
    "\tprint(\"activation={:.4f}; predicted_label={}, true_label={}\"\n",
    "          .format(float(activation[0]), label, float(f_y.iloc[i]) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_precision(X,W,y):\n",
    "    prediction = sigmoid_activation(X.dot(W));\n",
    "    error = (np.sign( np.multiply(prediction-0.5,y-0.5) )+1)/2;\n",
    "#     return np.mean(error);\n",
    "    return np.mean(error);\n",
    "#     return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.992\n",
       "dtype: float64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_precision(f_X,f_W,f_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGSCAYAAAA4v2GGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbca5a4f6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# construct a figure that plots the loss over time\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(0, epochs), lossHistory)\n",
    "fig.suptitle(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
